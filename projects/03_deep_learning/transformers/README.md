# üìù TransformerIMDb: Text Classification via Custom Transformer (PyTorch)

This project implements a simple Transformer encoder from scratch and fine-tunes it on the **IMDb** movie review dataset to demonstrate transformer-based text classification.

---

## üõ†Ô∏è Technologies Used

- **Python** ‚Äì Programming language  
- **PyTorch** ‚Äì Deep learning framework  
- **torchtext / datasets** ‚Äì Data loading and preprocessing  
- **transformers** ‚Äì Tokenization (BERT tokenizer)  
- **NumPy & pandas** ‚Äì Data manipulation  
- **tqdm** ‚Äì Progress bars for training loops  

---

## ‚ñ∂Ô∏è How to Run the Project

1. **Clone the repository**  
   ```bash
   git clone https://github.com/MammadovN/transformer_imdb_classification.git
   cd transformer_imdb_classification
2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
3. **Launch training script**
   ```bash
   jupyter notebook transformer_imdb.ipynb

---

## üìÇ Project Description

### Dataset
- Uses `datasets.load_dataset("imdb")` to fetch and split into train/test.  
- Samples a subset (1,000 train / 200 test) for quick experimentation.

### Tokenization & Preprocessing
- BERT‚Äôs `bert-base-uncased` tokenizer  
- Padding/truncation to length 128  
- Returns `input_ids` and `attention_mask` tensors

### DataLoader
- Batch size: 32  
- Shuffle training data, 2 worker processes  
- Uses PyTorch `DataLoader` for batching

### Model
- **Embedding Layer**: `nn.Embedding(vocab_size, d_model)`  
- **Positional Encoding**: Sine & cosine positional embeddings  
- **Encoder Layers**:  
  - Multi-Head Self-Attention (8 heads, `d_k = d_model/8`)  
  - Feed-Forward (linear ‚Üí ReLU ‚Üí linear)  
  - LayerNorm & residual connections  
- **Classifier Head**: Global average pooling ‚Üí `nn.Linear(d_model, 2)`

### Training Loop
- Optimizer: `Adam(lr=1e-3)`  
- Loss: `CrossEntropyLoss`  
- 3 epochs, prints average loss per epoch

### Device
- Automatically uses GPU if available  

---

## ‚úÖ Steps for Analysis

| Step                 | Description                                                                                      |
|----------------------|--------------------------------------------------------------------------------------------------|
| **Data Download**    | `load_dataset("imdb")` downloads and splits IMDb into train/test sets.                           |
| **Tokenization**     | BERT tokenizer pads/truncates to max length 128, returns `input_ids` & `attention_mask`.          |
| **DataLoader Setup** | Create `DataLoader` objects (batch_size=32, shuffle train, num_workers=2).                       |
| **Model Definition** | Build `SimpleTransformer`: embedding, positional encoding, encoder layers, and linear classifier.|
| **Attention**        | Implement Scaled Dot-Product attention with mask handling and dropout.                            |
| **Training**         | Loop over epochs: forward ‚Üí loss ‚Üí backward ‚Üí optimizer step, track average loss.                 |
| **Evaluation**       | Compute accuracy on test subset after training completes.                                        |

---

## üß† Model Architecture Overview

```text
Input: [batch_size, seq_len] token IDs
  ‚îî‚îÄ‚ñ∫ Embedding Layer (vocab_size ‚Üí d_model)
       ‚îî‚îÄ‚ñ∫ Positional Encoding added
            ‚îî‚îÄ‚ñ∫ N √ó [Multi-Head Self-Attention + Feed-Forward + LayerNorm]
                 ‚îî‚îÄ‚ñ∫ GlobalAveragePool (over seq_len)
                      ‚îî‚îÄ‚ñ∫ Linear(d_model ‚Üí 2) logits

