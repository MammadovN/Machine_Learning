# ğŸ“š TextGenRNN: Character-Level Text Generation with LSTM (PyTorch)

This project trains a **character-level LSTM-based RNN** to generate new text in the style of **Shakespeare**, based on a small dataset.  
The example workflow uses the openly available **Tiny Shakespeare** dataset.

---

## ğŸ›  Technologies Used

- **Python** â€“ Programming language  
- **PyTorch** â€“ Deep-learning framework  
- **Matplotlib** â€“ Plotting training and validation loss  
- **Requests** â€“ Dataset fetching from URL  
- **NumPy** â€“ Data manipulation  
- **tqdm** â€“ Progress bars during training  

---

## â–¶ï¸ How to Run the Project
1. **Clone the repository**  
   
bash
   git clone https://github.com/your-username/Text_Generation_Shakespeare.git
   cd SentimentRNN
2. **Install dependencies**
bash
   pip install -r requirements.txt
3. **Run the training script**
   
bash
   jupyter Text_Generation_Shakespeare.ipynb

# ğŸ“‚ Project Description

The pipeline encodes Shakespeareâ€™s text into integer sequences, trains a character-level LSTM model, and generates new text character-by-character.

- Download Tiny Shakespeare dataset (~1 MB, ~1 million characters)  
- Build character-level vocabulary mapping each unique character to an index  
- Encode the full text corpus into integer sequences  
- Prepare batches of input/output sequences for training  
- Model â€“ Embedding â†’ Single-layer LSTM â†’ Fully-Connected â†’ Softmax over characters  
- Train using CrossEntropyLoss and Adam optimizer  
- Generate novel text from a given seed string  

## âœ… Steps for Analysis

| Step               | Description                                                        |
|--------------------|--------------------------------------------------------------------|
| Data Loading       | Downloads Tiny Shakespeare via `requests.get()`.                   |
| Vocabulary Creation| Maps unique characters to indices (and vice-versa).                |
| Encoding           | Full text is converted into a tensor of integers.                  |
| Batch Preparation  | `get_batch()` samples random sequences for training.               |
| Model Architecture | Embedding â†’ LSTM â†’ Fully Connected output layer.                   |
| Training Loop      | Tracks training and validation loss.                               |
| Validation         | Evaluates loss every few steps during training.                    |
| Text Generation    | `generate_text()` produces novel text character-by-character.      |

## ğŸ§  Model Architecture Overview

```text
Input (batch_size, block_size) â”€â–º Embedding (vocab_size Ã— embed_dim)
                                  â”‚
                                  â–¼
                        Single-layer LSTM
                          hidden_dim = 512
                                  â”‚
                                  â–¼
                        Fully-Connected (vocab_size)
                                  â”‚
                                  â–¼
                       Softmax over next character
