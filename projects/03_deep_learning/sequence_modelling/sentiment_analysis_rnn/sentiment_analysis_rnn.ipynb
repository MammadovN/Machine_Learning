{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNbPt+R0ynBTyWe9PTkRXVc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MammadovN/Machine_Learning/blob/main/projects/03_deep_learning/sequence_modelling/sentiment_analysis_rnn/sentiment_analysis_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAcQxWXM9qpM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Device check\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U nltk\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=False)\n",
        "nltk.download('stopwords', quiet=False)\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "dTBF8TMjAMl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing functions\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Prepare text for an RNN: convert to lowercase, remove punctuation, apply stemming.\"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation and digits\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Tokenise\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop-words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def build_vocab(texts, max_words: int = 10_000):\n",
        "    \"\"\"Build a vocabulary from all texts.\"\"\"\n",
        "    all_words = []\n",
        "    for text in texts:\n",
        "        all_words.extend(text)\n",
        "\n",
        "    word_counter = Counter(all_words)\n",
        "    vocab = {\n",
        "        '<PAD>': 0,  # Special token for padding\n",
        "        '<UNK>': 1,  # Special token for unknown words\n",
        "    }\n",
        "\n",
        "    # Add the most frequent words to the vocabulary\n",
        "    for word, _ in word_counter.most_common(max_words - 2):\n",
        "        vocab[word] = len(vocab)\n",
        "\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def encode_text(text, vocab, max_length: int = 100):\n",
        "    \"\"\"Convert a token list into a sequence of integers.\"\"\"\n",
        "    encoded = [vocab.get(word, vocab['<UNK>']) for word in text]\n",
        "\n",
        "    # Pad or truncate to a fixed length\n",
        "    if len(encoded) < max_length:\n",
        "        encoded += [vocab['<PAD>']] * (max_length - len(encoded))\n",
        "    else:\n",
        "        encoded = encoded[:max_length]\n",
        "\n",
        "    return encoded\n"
      ],
      "metadata": {
        "id": "GYJqIY9T-DB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vocab, max_length: int = 100):\n",
        "        \"\"\"\n",
        "        Args\n",
        "        ----\n",
        "        texts : list[list[str]]\n",
        "            Pre-tokenised texts.\n",
        "        labels : list[int] | list[float]\n",
        "            Ground-truth sentiment labels.\n",
        "        vocab : dict[str, int]\n",
        "            Word-to-index mapping.\n",
        "        max_length : int, optional\n",
        "            Sequence length to pad/truncate to.\n",
        "        \"\"\"\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.vocab = vocab\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Return dataset size.\"\"\"\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        \"\"\"Return one (encoded_text, label) pair as tensors.\"\"\"\n",
        "        encoded_text = encode_text(self.texts[idx], self.vocab, self.max_length)\n",
        "        return (\n",
        "            torch.tensor(encoded_text, dtype=torch.long),\n",
        "            torch.tensor(self.labels[idx], dtype=torch.float),\n",
        "        )\n"
      ],
      "metadata": {
        "id": "PnU9oA9x-6Tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN Model\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embedding_dim: int,\n",
        "        hidden_dim: int,\n",
        "        output_dim: int,\n",
        "        n_layers: int,\n",
        "        bidirectional: bool,\n",
        "        dropout: float,\n",
        "    ):\n",
        "        super(SentimentRNN, self).__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=n_layers,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout if n_layers > 1 else 0,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Fully connected layer\n",
        "        # If using a bidirectional LSTM, multiply hidden_dim by 2\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text shape: [batch size, seq length]\n",
        "\n",
        "        # Pass through the embedding layer\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded shape: [batch size, seq length, embedding dim]\n",
        "\n",
        "        # Pass through the LSTM layer\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        # output shape : [batch size, seq length, hidden dim * num directions]\n",
        "        # hidden shape : [num layers * num directions, batch size, hidden dim]\n",
        "\n",
        "        if self.lstm.bidirectional:\n",
        "            # If bidirectional, concatenate the final forward & backward hidden states\n",
        "            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "        else:\n",
        "            hidden = hidden[-1, :, :]\n",
        "        # hidden shape: [batch size, hidden dim * num directions]\n",
        "\n",
        "        # Apply dropout\n",
        "        hidden = self.dropout(hidden)\n",
        "\n",
        "        # Pass through the output layer\n",
        "        return self.fc(hidden)\n"
      ],
      "metadata": {
        "id": "vh8IRJ4V--4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model-training function\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                val_loader,\n",
        "                criterion,\n",
        "                optimizer,\n",
        "                n_epochs: int = 5):\n",
        "    best_val_loss = float(\"inf\")\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs,  val_accs  = [], []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        # Set model to training mode\n",
        "        model.train()\n",
        "        epoch_loss, epoch_acc = 0.0, 0.0\n",
        "\n",
        "        for text_batch, label_batch in tqdm(\n",
        "            train_loader, desc=f\"Epoch {epoch + 1}/{n_epochs}\"\n",
        "        ):\n",
        "            text_batch  = text_batch.to(device)\n",
        "            label_batch = label_batch.to(device)\n",
        "\n",
        "            # Reset gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            predictions = model(text_batch).squeeze(1)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(predictions, label_batch)\n",
        "\n",
        "            # Back-propagation and optimisation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update metrics\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Classification accuracy\n",
        "            predicted_labels = torch.round(torch.sigmoid(predictions))\n",
        "            correct = (predicted_labels == label_batch).float().sum()\n",
        "            epoch_acc += correct.item() / len(label_batch)\n",
        "\n",
        "        # Epoch results\n",
        "        train_loss = epoch_loss / len(train_loader)\n",
        "        train_acc  = epoch_acc  / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), \"best_sentiment_model.pt\")\n",
        "            print(\"Model saved!\")\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs\n"
      ],
      "metadata": {
        "id": "ZJLiSRZM_J29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model-evaluation function\n",
        "def evaluate_model(model, data_loader, criterion):\n",
        "    \"\"\"\n",
        "    Evaluate the model on a given DataLoader.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple[float, float]\n",
        "        (average_loss, average_accuracy)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss, epoch_acc = 0.0, 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text_batch, label_batch in data_loader:\n",
        "            text_batch  = text_batch.to(device)\n",
        "            label_batch = label_batch.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            predictions = model(text_batch).squeeze(1)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(predictions, label_batch)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Classification accuracy\n",
        "            predicted_labels = torch.round(torch.sigmoid(predictions))\n",
        "            correct = (predicted_labels == label_batch).float().sum()\n",
        "            epoch_acc += correct.item() / len(label_batch)\n",
        "\n",
        "    return epoch_loss / len(data_loader), epoch_acc / len(data_loader)\n"
      ],
      "metadata": {
        "id": "6iGX5r1I_Slj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test function\n",
        "def test_model(model, test_loader):\n",
        "    \"\"\"\n",
        "    Evaluate the trained model on the test set.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple[float, str, np.ndarray, list]\n",
        "        accuracy,\n",
        "        classification report (str),\n",
        "        confusion matrix (ndarray),\n",
        "        list of predicted labels\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actual_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text_batch, label_batch in test_loader:\n",
        "            text_batch = text_batch.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(text_batch).squeeze(1)\n",
        "            predicted_labels = torch.round(torch.sigmoid(outputs))\n",
        "\n",
        "            predictions.extend(predicted_labels.cpu().numpy())\n",
        "            actual_labels.extend(label_batch.cpu().numpy())\n",
        "\n",
        "    # Performance metrics\n",
        "    accuracy = accuracy_score(actual_labels, predictions)\n",
        "    report = classification_report(actual_labels, predictions)\n",
        "    conf_matrix = confusion_matrix(actual_labels, predictions)\n",
        "\n",
        "    return accuracy, report, conf_matrix, predictions\n"
      ],
      "metadata": {
        "id": "cg20aNc0_XKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict sentiment for a new text\n",
        "def predict_sentiment(model, text, vocab, max_length: int = 100):\n",
        "    \"\"\"\n",
        "    Generate a sentiment prediction for an unseen text.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        Trained sentiment-analysis model.\n",
        "    text : str\n",
        "        Raw input sentence or document.\n",
        "    vocab : dict[str, int]\n",
        "        Word-to-index mapping.\n",
        "    max_length : int, optional\n",
        "        Sequence length used during training (for padding / truncation).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        {\n",
        "            'sentiment': \"Positive\" | \"Negative\",\n",
        "            'score': float,          # probability in [0, 1]\n",
        "            'tokens': list[str]      # pre-processed tokens\n",
        "        }\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Pre-process the input text\n",
        "    tokens = preprocess_text(text)\n",
        "    encoded = encode_text(tokens, vocab, max_length)\n",
        "    tensor  = torch.tensor(encoded).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction = torch.sigmoid(model(tensor).squeeze(1))\n",
        "\n",
        "    sentiment_score = prediction.item()\n",
        "    sentiment = \"Positive\" if sentiment_score >= 0.5 else \"Negative\"\n",
        "\n",
        "    return {\n",
        "        \"sentiment\": sentiment,\n",
        "        \"score\": sentiment_score,\n",
        "        \"tokens\": tokens,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "G92rVTWN_a1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main driver function\n",
        "def run_sentiment_analysis(\n",
        "    data_path,\n",
        "    test_size: float = 0.2,\n",
        "    val_size: float = 0.1,\n",
        "    max_words: int = 10_000,\n",
        "    max_length: int = 100,\n",
        "    embedding_dim: int = 100,\n",
        "    hidden_dim: int = 256,\n",
        "    n_layers: int = 2,\n",
        "    bidirectional: bool = True,\n",
        "    dropout: float = 0.5,\n",
        "    batch_size: int = 64,\n",
        "    n_epochs: int = 5,\n",
        "    learning_rate: float = 0.001,\n",
        "):\n",
        "    print(\"Loading data …\")\n",
        "    # Load the dataset (CSV format assumed)\n",
        "    df = pd.read_csv(data_path)\n",
        "\n",
        "    # Prepare the data\n",
        "    print(\"Pre-processing text …\")\n",
        "    texts = df[\"text\"].values\n",
        "    labels = df[\"label\"].values  # 0: negative, 1: positive\n",
        "\n",
        "    # Text preprocessing\n",
        "    processed_texts = [preprocess_text(text) for text in texts]\n",
        "\n",
        "    # Split the data\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "        processed_texts, labels, test_size=test_size, random_state=42\n",
        "    )\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_val,\n",
        "        y_train_val,\n",
        "        test_size=val_size / (1 - test_size),\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "    print(f\"Training set:     {len(X_train)} samples\")\n",
        "    print(f\"Validation set:   {len(X_val)} samples\")\n",
        "    print(f\"Test set:         {len(X_test)} samples\")\n",
        "\n",
        "    # Build the vocabulary\n",
        "    vocab = build_vocab(X_train, max_words)\n",
        "    vocab_size = len(vocab)\n",
        "    print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "    # Create Dataset and DataLoader objects\n",
        "    train_dataset = SentimentDataset(X_train, y_train, vocab, max_length)\n",
        "    val_dataset   = SentimentDataset(X_val,   y_val,   vocab, max_length)\n",
        "    test_dataset  = SentimentDataset(X_test,  y_test,  vocab, max_length)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size)\n",
        "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size)\n",
        "\n",
        "    # Build the model\n",
        "    model = SentimentRNN(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=embedding_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        output_dim=1,\n",
        "        n_layers=n_layers,\n",
        "        bidirectional=bidirectional,\n",
        "        dropout=dropout,\n",
        "    ).to(device)\n",
        "\n",
        "    print(\"Model architecture:\")\n",
        "    print(model)\n",
        "\n",
        "    # Optimiser and loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Training model …\")\n",
        "    train_losses, val_losses, train_accs, val_accs = train_model(\n",
        "        model, train_loader, val_loader, criterion, optimizer, n_epochs\n",
        "    )\n",
        "\n",
        "    # Load the best model weights\n",
        "    model.load_state_dict(torch.load(\"best_sentiment_model.pt\"))\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    print(\"Evaluating on test set …\")\n",
        "    accuracy, report, conf_matrix, _ = test_model(model, test_loader)\n",
        "\n",
        "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
        "    print(\"Classification report:\")\n",
        "    print(report)\n",
        "    print(\"Confusion matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label=\"Train Loss\")\n",
        "    plt.plot(val_losses,   label=\"Val Loss\")\n",
        "    plt.title(\"Loss per Epoch\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label=\"Train Acc\")\n",
        "    plt.plot(val_accs,   label=\"Val Acc\")\n",
        "    plt.title(\"Accuracy per Epoch\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"sentiment_training.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Example predictions\n",
        "    print(\"\\nExample predictions:\")\n",
        "    example_texts = [\n",
        "        \"This movie was absolutely terrible. I hated every minute of it.\",\n",
        "        \"I loved this product! It works exactly as described and the quality is amazing.\",\n",
        "        \"The service was okay, nothing special but not bad either.\",\n",
        "    ]\n",
        "\n",
        "    for text in example_texts:\n",
        "        result = predict_sentiment(model, text, vocab, max_length)\n",
        "        print(f\"Text: {text}\")\n",
        "        print(f\"Sentiment: {result['sentiment']} (Score: {result['score']:.4f})\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    return model, vocab\n"
      ],
      "metadata": {
        "id": "3STWzGRn_fVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets\n"
      ],
      "metadata": {
        "id": "zEsVe-oxIS1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    \"\"\"\n",
        "    Sentiment140 verisetiyle tam uçtan-uca eğitim/deneme örneği.\n",
        "    - Veri kümesindeki 'sentiment' sütunu: 0 = negative, 4 = positive\n",
        "    - run_sentiment_analysis() CSV dosyası beklediği için küçük bir alt-kümeyi\n",
        "      pandas DataFrame'e dönüştürüp geçici CSV olarak kaydediyoruz.\n",
        "    \"\"\"\n",
        "    from datasets import load_dataset\n",
        "    import pandas as pd\n",
        "    from sklearn.utils import shuffle\n",
        "    import os\n",
        "\n",
        "    print(\"Downloading Sentiment140 …\")\n",
        "    # Bu adım ilk çalıştırmada ~80 MB indirir ve birkaç saniye sürebilir\n",
        "    ds = load_dataset(\"sentiment140\", split=\"train\")   # 1.6 M tweet\n",
        "\n",
        "    # ▸ Opsiyonel: Daha hızlı deneme için 100 000 satırlık alt-kümeyle çalış\n",
        "    #   Tamamını kullanmak istiyorsan next line'ı yorum satırı yap.\n",
        "    ds = ds.shuffle(seed=42).select(range(100_000))\n",
        "\n",
        "    # sentiment (0 or 4)  →  label (0 or 1)\n",
        "    df = pd.DataFrame({\n",
        "        \"text\": ds[\"text\"],\n",
        "        \"label\": [0 if s == 0 else 1 for s in ds[\"sentiment\"]]\n",
        "    })\n",
        "\n",
        "    # Shuffle & save to a temporary CSV\n",
        "    df = shuffle(df, random_state=42)\n",
        "    csv_path = \"sentiment140_binary.csv\"\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    # Train / evaluate the model\n",
        "    model, vocab = run_sentiment_analysis(\n",
        "        csv_path,\n",
        "        n_epochs=5,        # Eğitimi istediğin gibi ayarlayabilirsin\n",
        "        batch_size=64,\n",
        "        max_words=10_000,  # En sık 10 000 kelime\n",
        "        max_length=100\n",
        "    )\n",
        "\n",
        "    # Temizlik (istersen CSV'yi saklayabilirsin)\n",
        "    os.remove(csv_path)\n"
      ],
      "metadata": {
        "id": "ICMbpCDYDS2B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}