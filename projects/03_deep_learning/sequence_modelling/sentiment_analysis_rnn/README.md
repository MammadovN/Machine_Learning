# ğŸ’¬ SentimentRNN: Text Sentiment Classification with Bidirectional LSTM (PyTorch)

This project trains a **Bidirectional LSTM-based RNN** to predict the **sentiment** (positive / negative) of short texts such as tweets, reviews, or comments.  
The example workflow below uses the openly available **Sentiment140** dataset, fetched in one line with ğŸ¤— *datasets*.

---

## ğŸ›  Technologies Used

- **Python** â€“ Programming language  
- **PyTorch** â€“ Deep-learning framework  
- **torchvision** â€“ (optional) utilities for embeddings  
- **ğŸ¤— datasets** â€“ Instant access to Sentiment140 / IMDb / Yelp, etc.  
- **NLTK** â€“ Tokenisation, stop-words, stemming  
- **scikit-learn** â€“ Data split & evaluation metrics  
- **pandas & NumPy** â€“ Data manipulation  
- **matplotlib** â€“ Training curves  
- **tqdm** â€“ Progress bars  

---

## â–¶ï¸ How to Run the Project

1. **Clone the repository**  
   ```bash
   git clone https://github.com/your-username/sentiment_analysis_rnn.git
   cd SentimentRNN
2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
3. **Run the training script**
   ```bash
   jupyter sentiment_analysis_rnn.ipynb

## ğŸ“‚ Project Description  

The pipeline converts raw text into integer sequences, feeds them into a bidirectional LSTM, and outputs a **binary sentiment score**.

1. **Download & split Sentiment140** â†’ train / validation / test  
2. **Pre-processing** â€“ lowercase â†’ remove punctuation/digits â†’ tokenise â†’ remove stop-words â†’ stemming  
3. **Build vocabulary** of the *N* most frequent tokens (default: 10 000)  
4. **Encode & pad** each text to a fixed length (default: 100 tokens)  
5. **Model** â€“ Embedding â†’ 2-layer **Bi-LSTM** â†’ Dropout â†’ Fully-Connected â†’ Sigmoid  
6. **Train** with `BCEWithLogitsLoss` + **Adam**, model checkpointing on best val-loss  
7. **Evaluate** accuracy, classification report & confusion matrix  
8. **Predict** sentiment for any new sentence via a helper function  

---

## âœ… Steps for Analysis  

| Step | Description |
|------|-------------|
| **Data Loading** | `load_dataset("sentiment140")` pulls tweets in *(text, sentiment)* format. |
| **Tokenisation & Cleaning** | Custom `preprocess_text()` uses NLTK. |
| **Vocabulary** | `build_vocab()` assigns indices and adds `<PAD>` / `<UNK>`. |
| **Dataset / DataLoader** | `SentimentDataset` returns `(tensor, label)` pairs. |
| **Model Architecture** | See diagram below. |
| **Training Loop** | `train_model()` tracks loss & accuracy each epoch and saves best weights. |
| **Validation** | `evaluate_model()` computes metrics on the hold-out set. |
| **Testing** | `test_model()` prints accuracy, detailed report, confusion matrix. |
| **Inference** | `predict_sentiment()` returns sentiment & probability for raw sentences. |

---

## ğŸ§  Model Architecture Overview  

```text
Input (batch, seq_len) â”€â–º Embedding (vocab_size Ã— 100)
                         â”‚
                         â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      2-layer Bidirectional LSTM           â”‚
        â”‚   hidden_dim = 256, dropout = 0.5         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
  Concatenate last fwd & bwd hidden states
                         â”‚
                         â–¼
                Dropout (0.5)
                         â”‚
                         â–¼
                Fully-Connected (1)
                         â”‚
                         â–¼
                 Sigmoid â†’ p(Positive)

