{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNlxLYdaVNzqHBHX+iQquzP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MammadovN/Machine_Learning/blob/main/projects/03_deep_learning/neural_style_transfer/style_transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBGpHneOBm6h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import copy\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# Check whether a GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ⬆️ 1. Upload helper ----------------------------------------------------------\n",
        "def upload_images():\n",
        "    \"\"\"Let the user upload two files, then determine which is content/style.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple[str, str]\n",
        "        A pair of paths: (content_path, style_path)\n",
        "    \"\"\"\n",
        "    uploaded = files.upload()\n",
        "    content_path, style_path = None, None\n",
        "\n",
        "    # Auto-detect by filename\n",
        "    for filename in uploaded.keys():\n",
        "        fname = filename.lower()\n",
        "        if \"content\" in fname:\n",
        "            content_path = filename\n",
        "        elif \"style\" in fname:\n",
        "            style_path = filename\n",
        "\n",
        "    # Ask the user if auto-detection failed\n",
        "    if content_path is None or style_path is None:\n",
        "        print(\n",
        "            \"Please indicate which of the uploaded files is the CONTENT image \"\n",
        "            \"and which is the STYLE image:\"\n",
        "        )\n",
        "        file_list = list(uploaded.keys())\n",
        "        for i, file in enumerate(file_list):\n",
        "            print(f\"{i}: {file}\")\n",
        "\n",
        "        content_idx = int(input(\"Number of the content image: \"))\n",
        "        style_idx   = int(input(\"Number of the style image  : \"))\n",
        "\n",
        "        content_path = file_list[content_idx]\n",
        "        style_path   = file_list[style_idx]\n",
        "\n",
        "    print(f\"Content image: {content_path}\")\n",
        "    print(f\"Style image  : {style_path}\")\n",
        "\n",
        "    return content_path, style_path\n",
        "\n",
        "\n",
        "# ⬆️ 2. Image-loading / preprocessing -----------------------------------------\n",
        "def load_image(img_path, max_size: int = 400, shape=None):\n",
        "    \"\"\"Load an image file and transform it into a pre-processed tensor.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    img_path : str\n",
        "        File path of the image to load.\n",
        "    max_size : int, optional\n",
        "        Maximum dimension (width or height). Larger images are downsized.\n",
        "    shape : tuple[int, int] | None, optional\n",
        "        Force-resize to this shape instead of using `max_size`.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        A 4-D tensor (1, C, H, W) ready for VGG.\n",
        "    \"\"\"\n",
        "    image = Image.open(img_path)\n",
        "\n",
        "    # Decide new size\n",
        "    size = max_size if max(image.size) > max_size else max(image.size)\n",
        "    if shape is not None:\n",
        "        size = shape\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=(0.485, 0.456, 0.406),\n",
        "            std=(0.229, 0.224, 0.225)\n",
        "        ),\n",
        "    ])\n",
        "\n",
        "    # Convert to tensor and add batch dimension\n",
        "    image = transform(image)[:3, :, :].unsqueeze(0)\n",
        "    return image\n",
        "\n",
        "\n",
        "# ⬆️ 3. Tensor → displayable image --------------------------------------------\n",
        "def im_convert(tensor):\n",
        "    \"\"\"Convert a pre-processed tensor back to a NumPy image in [0, 1].\"\"\"\n",
        "    image = tensor.to(\"cpu\").clone().detach().numpy().squeeze()\n",
        "    image = image.transpose(1, 2, 0)  # CHW ➜ HWC\n",
        "    image = (\n",
        "        image * np.array((0.229, 0.224, 0.225)) +\n",
        "        np.array((0.485, 0.456, 0.406))\n",
        "    )\n",
        "    return np.clip(image, 0, 1)"
      ],
      "metadata": {
        "id": "Al2kasESB2Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContentStyleLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Compute the total loss (content + style) for neural style transfer.\n",
        "    Uses a pre-trained VGG-19 feature extractor.\n",
        "    \"\"\"\n",
        "    def __init__(self, style_img, content_img,\n",
        "                 content_weight: float = 1.0,\n",
        "                 style_weight: float = 1_000_000.0):\n",
        "        super().__init__()\n",
        "        self.style_img     = style_img\n",
        "        self.content_img   = content_img\n",
        "        self.content_weight = content_weight\n",
        "        self.style_weight   = style_weight\n",
        "\n",
        "        # Use VGG-19 convolutional features\n",
        "        self.model = models.vgg19(pretrained=True).features.eval()\n",
        "        self.content_layers = ['conv_4']\n",
        "        self.style_layers   = [\n",
        "            'conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5'\n",
        "        ]\n",
        "\n",
        "        # Gram-matrix helper\n",
        "        self.gram = GramMatrix()\n",
        "\n",
        "    def forward(self, input_img):\n",
        "        content_loss = 0.0\n",
        "        style_loss   = 0.0\n",
        "\n",
        "        # Pre-compute style and content targets\n",
        "        style_features   = self.get_features(self.style_img)\n",
        "        content_features = self.get_features(self.content_img)\n",
        "\n",
        "        # Features of the current (generated) image\n",
        "        input_features = self.get_features(input_img)\n",
        "\n",
        "        # Content loss\n",
        "        for layer in self.content_layers:\n",
        "            content_loss += F.mse_loss(\n",
        "                input_features[layer],\n",
        "                content_features[layer]\n",
        "            )\n",
        "\n",
        "        # Style loss\n",
        "        for layer in self.style_layers:\n",
        "            input_gram  = self.gram(input_features[layer])\n",
        "            style_gram  = self.gram(style_features[layer])\n",
        "            style_loss += F.mse_loss(input_gram, style_gram)\n",
        "\n",
        "        # Total loss = weighted sum\n",
        "        total_loss = (\n",
        "            self.content_weight * content_loss +\n",
        "            self.style_weight   * style_loss\n",
        "        )\n",
        "        return total_loss\n",
        "\n",
        "    def get_features(self, image):\n",
        "        \"\"\"Extract selected layer activations from VGG-19.\"\"\"\n",
        "        features = {}\n",
        "        layer_name_mapping = {\n",
        "            '0':  'conv_1',\n",
        "            '5':  'conv_2',\n",
        "            '10': 'conv_3',\n",
        "            '19': 'conv_4',\n",
        "            '28': 'conv_5',\n",
        "        }\n",
        "\n",
        "        x = image\n",
        "        for name, layer in self.model._modules.items():\n",
        "            x = layer(x)\n",
        "            if name in layer_name_mapping:\n",
        "                features[layer_name_mapping[name]] = x\n",
        "        return features"
      ],
      "metadata": {
        "id": "mpG5Lh4CCB34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Gram-matrix helper\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class GramMatrix(nn.Module):\n",
        "    def forward(self, input):\n",
        "        batch_size, f_maps, h, w = input.size()\n",
        "        features = input.view(batch_size, f_maps, h * w)\n",
        "        gram = torch.bmm(features, features.transpose(1, 2))\n",
        "\n",
        "        # Normalise\n",
        "        gram = gram.div(f_maps * h * w)\n",
        "        return gram\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Style-transfer routine\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def style_transfer(\n",
        "    content_path,\n",
        "    style_path,\n",
        "    num_steps: int = 2_000,\n",
        "    content_weight: float = 1.0,\n",
        "    style_weight: float = 1_000_000.0,\n",
        "):\n",
        "    # Load images\n",
        "    content = load_image(content_path).to(device)\n",
        "    style   = load_image(style_path, shape=content.shape[-2:]).to(device)\n",
        "\n",
        "    # Image to be optimised (start from a copy of the content image)\n",
        "    generated = content.clone().requires_grad_(True).to(device)\n",
        "\n",
        "    # Optimiser\n",
        "    optimizer = optim.Adam([generated], lr=0.003)\n",
        "\n",
        "    # Loss function\n",
        "    loss_fn = ContentStyleLoss(style, content, content_weight, style_weight).to(device)\n",
        "\n",
        "    # Progress bar (tqdm in notebooks, plain range otherwise)\n",
        "    try:\n",
        "        from tqdm.notebook import tqdm\n",
        "        iterator = tqdm(range(num_steps))\n",
        "    except ImportError:\n",
        "        iterator = range(num_steps)\n",
        "\n",
        "    # Training loop\n",
        "    for step in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(generated)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 200 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "            # Show intermediate results\n",
        "            plt.figure(figsize=(15, 5))\n",
        "\n",
        "            plt.subplot(1, 3, 1)\n",
        "            plt.imshow(im_convert(content))\n",
        "            plt.title(\"Content\")\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "            plt.subplot(1, 3, 2)\n",
        "            plt.imshow(im_convert(style))\n",
        "            plt.title(\"Style\")\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "            plt.subplot(1, 3, 3)\n",
        "            plt.imshow(im_convert(generated))\n",
        "            plt.title(f\"Generated (Step {step})\")\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    return generated"
      ],
      "metadata": {
        "id": "FiyFgHadFN-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Save the output image and download it from Colab\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def save_and_download_image(tensor, filename: str = \"style_transfer_result.jpg\"):\n",
        "    final_img = im_convert(tensor)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(final_img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.savefig(filename, bbox_inches=\"tight\", pad_inches=0.1)\n",
        "    plt.show()\n",
        "\n",
        "    # Download from Colab\n",
        "    files.download(filename)\n",
        "    print(f\"Style transfer completed! The result was downloaded as '{filename}'.\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Top-level helper\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def run_style_transfer():\n",
        "    print(\"Please upload one CONTENT image and one STYLE image.\")\n",
        "    print(\"(Tip: If the filenames contain the words 'content' and 'style', \"\n",
        "          \"they will be detected automatically.)\")\n",
        "\n",
        "    content_path, style_path = upload_images()\n",
        "\n",
        "    # Get parameters from the user\n",
        "    print(\"\\nAdjust style-transfer parameters:\")\n",
        "    num_steps = int(input(\"Number of iterations (suggested: 1000–3000) [2000]: \") or 2000)\n",
        "    content_weight = float(input(\"Content weight (suggested: 1) [1]: \") or 1)\n",
        "    style_weight   = float(input(\"Style weight   (suggested: 1,000,000) [1000000]: \") or 1_000_000)\n",
        "\n",
        "    print(\"\\nStarting style transfer …\")\n",
        "    result = style_transfer(\n",
        "        content_path,\n",
        "        style_path,\n",
        "        num_steps=num_steps,\n",
        "        content_weight=content_weight,\n",
        "        style_weight=style_weight,\n",
        "    )\n",
        "\n",
        "    # Save and download the final image\n",
        "    save_and_download_image(result)\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Entry point\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "print(\"Neural Style Transfer — Google Colab\")\n",
        "print(\"=\" * 40)\n",
        "print(\"This program recreates a CONTENT image in the STYLE of another image.\\n\")\n",
        "run_style_transfer()\n"
      ],
      "metadata": {
        "id": "B2ZStsvXkfNB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}