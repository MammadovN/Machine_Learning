{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNw8n/yXsDlDrTKoE0cupJq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MammadovN/Machine_Learning/blob/main/projects/03_deep_learning/transfer_learning/transfer_learning_classification/transfer_learning_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9rNSRDy1fSGE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loaders(batch_size=32):\n",
        "    \"\"\"\n",
        "    Uses CIFAR-10 as the dataset, applying necessary transforms.\n",
        "    Returns dataloaders dict, dataset sizes, and class names.\n",
        "    \"\"\"\n",
        "    # Data augmentation and normalization for training\n",
        "    # Just normalization for validation/test\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(224, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    train_ds = datasets.CIFAR10(root='./data', train=True,\n",
        "                                download=True, transform=train_transform)\n",
        "    val_ds   = datasets.CIFAR10(root='./data', train=False,\n",
        "                                download=True, transform=test_transform)\n",
        "    # For simplicity, use val_ds as test set as well\n",
        "    dataloaders = {\n",
        "        'train': DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2),\n",
        "        'val':   DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=2),\n",
        "        'test':  DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    }\n",
        "    sizes = {'train': len(train_ds), 'val': len(val_ds), 'test': len(val_ds)}\n",
        "    class_names = train_ds.classes\n",
        "    return dataloaders, sizes, class_names"
      ],
      "metadata": {
        "id": "8lSL1i16fY-J"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(num_classes, feature_extract=True):\n",
        "    \"\"\"\n",
        "    Loads pretrained ResNet50, freezes conv layers if feature_extract,\n",
        "    and replaces the final FC layer for num_classes.\n",
        "    \"\"\"\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    if feature_extract:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, num_classes)\n",
        "    return model"
      ],
      "metadata": {
        "id": "wN8i_BInfdgA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloaders, sizes, criterion, optimizer, device, num_epochs=10):\n",
        "    \"\"\"\n",
        "    Trains the model, returns best model (by val accuracy).\n",
        "    \"\"\"\n",
        "    best_weights = model.state_dict()\n",
        "    best_acc = 0.0\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "        for phase in ['train', 'val']:\n",
        "            model.train() if phase=='train' else model.eval()\n",
        "            running_loss, running_corrects = 0.0, 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(phase=='train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    if phase=='train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds==labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / sizes[phase]\n",
        "            print(f\" {phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "            if phase=='val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_weights = model.state_dict()\n",
        "\n",
        "    model.load_state_dict(best_weights)\n",
        "    return model"
      ],
      "metadata": {
        "id": "CBW6s4B2ff-V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluates on test set and returns accuracy.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    corrects, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            corrects += torch.sum(preds==labels.data).item()\n",
        "            total += labels.size(0)\n",
        "    return corrects / total"
      ],
      "metadata": {
        "id": "NFTC8yolfkFk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Hyperparameters\n",
        "    epochs = 10\n",
        "    batch_size = 32\n",
        "    lr = 1e-3\n",
        "    feature_extract = True\n",
        "    output_path = 'best_resnet50.pth'\n",
        "\n",
        "    # Prepare data\n",
        "    dataloaders, sizes, class_names = get_data_loaders(batch_size=batch_size)\n",
        "\n",
        "    # Build model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = build_model(len(class_names), feature_extract=feature_extract)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "\n",
        "    # Train & evaluate\n",
        "    best_model = train_model(model, dataloaders, sizes, criterion, optimizer, device, num_epochs=epochs)\n",
        "    test_acc = evaluate_model(best_model, dataloaders['test'], device)\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    # Save\n",
        "    torch.save({'model_state_dict': best_model.state_dict(),\n",
        "                'class_names': class_names}, output_path)\n"
      ],
      "metadata": {
        "id": "ty1TL8rTfm0F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0caQ5zO7fqnR",
        "outputId": "142bb57c-6d25-48e7-8802-d07f679088db"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.4MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 200MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            " train Loss: 0.7419 Acc: 0.7545\n",
            " val Loss: 0.6188 Acc: 0.7914\n",
            "Epoch 2/10\n",
            " train Loss: 0.6098 Acc: 0.7928\n",
            " val Loss: 0.5712 Acc: 0.8035\n",
            "Epoch 3/10\n",
            " train Loss: 0.5901 Acc: 0.7989\n",
            " val Loss: 0.5523 Acc: 0.8124\n",
            "Epoch 4/10\n",
            " train Loss: 0.5704 Acc: 0.8061\n",
            " val Loss: 0.5273 Acc: 0.8214\n",
            "Epoch 5/10\n",
            " train Loss: 0.5532 Acc: 0.8118\n",
            " val Loss: 0.5663 Acc: 0.8145\n",
            "Epoch 6/10\n",
            " train Loss: 0.5423 Acc: 0.8151\n",
            " val Loss: 0.5450 Acc: 0.8185\n",
            "Epoch 7/10\n",
            " train Loss: 0.5324 Acc: 0.8173\n",
            " val Loss: 0.4996 Acc: 0.8305\n",
            "Epoch 8/10\n",
            " train Loss: 0.5262 Acc: 0.8213\n",
            " val Loss: 0.5201 Acc: 0.8216\n",
            "Epoch 9/10\n",
            " train Loss: 0.5309 Acc: 0.8220\n",
            " val Loss: 0.5860 Acc: 0.8014\n",
            "Epoch 10/10\n",
            " train Loss: 0.5203 Acc: 0.8223\n",
            " val Loss: 0.5394 Acc: 0.8194\n",
            "Test Accuracy: 0.8194\n"
          ]
        }
      ]
    }
  ]
}