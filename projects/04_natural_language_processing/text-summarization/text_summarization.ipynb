{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOKzeUNxNYwziZ6GpkY7xkd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MammadovN/Machine_Learning/blob/main/projects/04_natural_language_processing/text-summarization/text_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate rouge_score"
      ],
      "metadata": {
        "id": "lBqiXfQaXNfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "MBvm5MEdXOdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "def preprocess(batch):\n",
        "    inputs = [\"summarize: \" + doc for doc in batch[\"article\"]]\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            batch[\"highlights\"],\n",
        "            max_length=150,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized = dataset.map(\n",
        "    preprocess,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ").shuffle(seed=42)"
      ],
      "metadata": {
        "id": "wYBDpwkVXRWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    # replace -100 in labels as pad_token_id for decoding\n",
        "    labels = [\n",
        "        [l if l != -100 else tokenizer.pad_token_id for l in label]\n",
        "        for label in labels\n",
        "    ]\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return {k: v * 100 for k, v in result.items()}"
      ],
      "metadata": {
        "id": "7IHiV5jVXWD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"model_output\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    report_to=\"none\",         # ← WandB & diğerleri devre dışı\n",
        "    logging_steps=100,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    predict_with_generate=True,\n",
        "    gradient_accumulation_steps=2,\n",
        "    fp16=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ],
      "metadata": {
        "id": "YkqG5MzQXZKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "QQ2ymJU5Xdq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", results)"
      ],
      "metadata": {
        "id": "04CQZckYXfx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(text, max_length=150, min_length=40, length_penalty=2.0, num_beams=4):\n",
        "    inputs = tokenizer(\n",
        "        \"summarize: \" + text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    ).to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        length_penalty=length_penalty,\n",
        "        num_beams=num_beams\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "ezRAKYy8XiPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = dataset[\"test\"][0]\n",
        "print(\"\\n--- DEMO ---\")\n",
        "print(\"Article:\\n\", sample[\"article\"])\n",
        "print(\"\\nTrue Summary:\\n\", sample[\"highlights\"])\n",
        "print(\"\\nPredicted Summary:\\n\", summarize(sample[\"article\"]))"
      ],
      "metadata": {
        "id": "xfEi-ICLXlJm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}